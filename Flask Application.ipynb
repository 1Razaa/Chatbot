{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e427000",
   "metadata": {},
   "source": [
    "# Flask Application For Chatbot "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1efc1d8",
   "metadata": {},
   "source": [
    "# Flask application creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e307da",
   "metadata": {},
   "source": [
    "The code app = Flask(__name__) creates a Flask application instance named app.\n",
    "\n",
    "Here's a brief explanation of what this line does:\n",
    "\n",
    "Flask: Flask is a class from the Flask framework that represents a Flask application. By creating an instance of this class, you can create and configure your Flask application.\n",
    "\n",
    "__name__: __name__ is a special Python variable that represents the name of the current module. When __name__ is used as the argument for the Flask class, it tells Flask to use the current module as the starting point for the application.\n",
    "\n",
    "By assigning Flask(__name__) to the variable app, you create a Flask application object that represents your web application. This app object can then be used to define routes, handle HTTP requests, and perform other tasks required to build a web application using Flask."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b10bb3",
   "metadata": {},
   "source": [
    "# Load the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e12da11",
   "metadata": {},
   "source": [
    "The code model = keras.models.load_model('cb_model') loads a pre-trained model using the Keras load_model function from TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a35a95",
   "metadata": {},
   "source": [
    "Here's a breakdown of the code:\n",
    "\n",
    "**with open('tokenizer.pickle', 'rb') as handle::** This line opens the file 'tokenizer.pickle' in read-binary mode ('rb'). The file contains the serialized tokenizer object.\n",
    "\n",
    "**tokenizer = pickle.load(handle):** This line uses the pickle.load() function to deserialize and load the tokenizer object from the opened file. The loaded tokenizer object is then assigned to the variable tokenizer.\n",
    "\n",
    "**with open('label_encoder.pickle', 'rb') as enc::** This line opens the file 'label_encoder.pickle' in read-binary mode ('rb'). The file contains the serialized label encoder object.\n",
    "\n",
    "**lbl_encoder = pickle.load(enc):** This line uses pickle.load() to deserialize and load the label encoder object from the file. The loaded label encoder object is assigned to the variable lbl_encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed2c482",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f4ea51",
   "metadata": {},
   "source": [
    "# Running the model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9db1a27",
   "metadata": {},
   "source": [
    "The @app.route('/') decorator associates the URL route '/' (the root URL) with the home() function. When a user visits the root URL, the home() function is executed.\n",
    "\n",
    "The home() function returns the result of rendering the HTML template 'index.html' using the render_template() function provided by Flask. It passes an empty list chats=[] as a parameter to the template. This allows you to pass data from your Python code to the HTML template for rendering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e09840c",
   "metadata": {},
   "source": [
    "The @app.route('/chat', methods=['POST']) decorator associates the URL route '/chat' with the chat() function. It specifies that the function should handle POST requests.\n",
    "\n",
    "The chat() function performs the following steps:\n",
    "\n",
    "1. Retrieves the value of the 'message' field from the submitted form data.\n",
    "2. Converts the user's text to a sequence of integers using tokenization.\n",
    "3. Pads the sequence to a fixed length.\n",
    "4. Uses the pre-trained model to predict the class probabilities for the padded sequence.\n",
    "5. Determines the predicted tag by finding the index of the highest probability.\n",
    "6. Loads intents data from the 'intents.json' file.\n",
    "7. Selects a random response associated with the predicted tag.\n",
    "8. Constructs a list of chat messages including the user's input and the bot's response.\n",
    "9. Renders the 'index.html' template, passing the chat messages as a parameter.\n",
    "The if __name__ == '__main__': block checks if the script is being executed as the main module, and if so, it runs the Flask application using app.run(), making the application accessible via a web server.\n",
    "\n",
    "Overall, this code defines a route handler for the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d97c479",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abdul Rehman Qureshi\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [02/Jan/2024 12:49:01] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [02/Jan/2024 12:49:01] \"GET /style.css HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [02/Jan/2024 12:49:02] \"GET /script.js HTTP/1.1\" 404 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 175ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [02/Jan/2024 12:49:55] \"POST /chat HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 39ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [02/Jan/2024 12:50:07] \"POST /chat HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 63ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [02/Jan/2024 12:50:31] \"POST /chat HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [02/Jan/2024 12:51:08] \"POST /chat HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [02/Jan/2024 12:51:38] \"POST /chat HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [02/Jan/2024 12:53:11] \"POST /chat HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 41ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [02/Jan/2024 12:53:28] \"POST /chat HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 41ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [02/Jan/2024 12:54:22] \"POST /chat HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 39ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [02/Jan/2024 12:54:31] \"POST /chat HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 40ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [02/Jan/2024 12:54:43] \"POST /chat HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [02/Jan/2024 12:55:10] \"POST /chat HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, request\n",
    "import json\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Configuration variables\n",
    "MODEL_FILE = 'cb_model_Alphaa'\n",
    "TOKENIZER_FILE = 'tokenizer.pickle'\n",
    "LABEL_ENCODER_FILE = 'label_encoder.pickle'\n",
    "DATA_FILE = 'Queries.json'\n",
    "MAX_SEQUENCE_LENGTH = 20\n",
    "\n",
    "# Load the trained model, tokenizer, and label encoder once when the app starts\n",
    "model = keras.models.load_model(MODEL_FILE)\n",
    "tokenizer = pickle.load(open(TOKENIZER_FILE, 'rb'))\n",
    "lbl_encoder = pickle.load(open(LABEL_ENCODER_FILE, 'rb'))\n",
    "\n",
    "# Function to perform speech recognition\n",
    "def recognize_speech():\n",
    "    recognizer = sr.Recognizer()\n",
    "    microphone = sr.Microphone()\n",
    "\n",
    "    with microphone as source:\n",
    "        print(\"Listening...\")\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "        audio = recognizer.listen(source)\n",
    "\n",
    "    try:\n",
    "        print(\"Recognizing...\")\n",
    "        text = recognizer.recognize_google(audio)\n",
    "        print(\"You said:\", text)\n",
    "        return text\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Could not understand audio\")\n",
    "        return None\n",
    "    except sr.RequestError as e:\n",
    "        print(f\"Could not request results from Google Speech Recognition service; {e}\")\n",
    "        return None\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template('index.html', chats=[])\n",
    "\n",
    "from flask import Flask, render_template, request, jsonify\n",
    "\n",
    "# ...\n",
    "\n",
    "@app.route('/chat', methods=['POST'])\n",
    "def chat():\n",
    "    if 'voice' in request.json:\n",
    "        prompt = recognize_speech()\n",
    "    else:\n",
    "        prompt = request.json['message']\n",
    "\n",
    "    if prompt is None:\n",
    "        return jsonify({'message': 'Invalid input'})\n",
    "\n",
    "    input_sequence = tokenizer.texts_to_sequences([prompt])\n",
    "    padded_sequence = keras.preprocessing.sequence.pad_sequences(input_sequence, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "    result = model.predict(padded_sequence)\n",
    "    predicted_tag = lbl_encoder.inverse_transform([np.argmax(result)])[0]\n",
    "\n",
    "    with open(DATA_FILE) as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    response = 'Sorry, I did not understand that.'  \n",
    "\n",
    "    for intent in data['intents']:\n",
    "        if intent['intent'] == predicted_tag:\n",
    "            response = np.random.choice(intent['responses'])\n",
    "            break\n",
    "\n",
    "    return jsonify({'message': response})\n",
    "\n",
    "\n",
    "    chats = [\n",
    "        {'user': True, 'message': prompt},\n",
    "        {'user': False, 'message': response}\n",
    "    ]\n",
    "    return render_template('index.html', chats=chats)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b750f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
